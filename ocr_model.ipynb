{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGFTlonP9WJX",
        "outputId": "10e2ff2d-8861-42a9-b026-59a82e9771da"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5_o3zkHR9Rj-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import joblib\n",
        "import pywt\n",
        "import random\n",
        "import numpy as np\n",
        "import cv2 as cv\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.fftpack import dct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "agS0mGhq9RkL"
      },
      "outputs": [],
      "source": [
        "#for binarization\n",
        "def binary_otsus(image, filter:int=1):\n",
        "    if len(image.shape) == 3:\n",
        "        gray_img = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
        "    else:\n",
        "        gray_img = image\n",
        "\n",
        "    # Otsus Binarization\n",
        "    if filter != 0:\n",
        "        blur = cv.GaussianBlur(gray_img, (3,3), 0)\n",
        "        binary_img = cv.threshold(blur, 0, 255, cv.THRESH_BINARY+cv.THRESH_OTSU)[1]\n",
        "    else:\n",
        "        binary_img = cv.threshold(gray_img, 0, 255, cv.THRESH_BINARY+cv.THRESH_OTSU)[1]\n",
        "\n",
        "    # Morphological Opening\n",
        "    # kernel = np.ones((3,3),np.uint8)\n",
        "    # clean_img = cv.morphologyEx(binary_img, cv.MORPH_OPEN, kernel)\n",
        "\n",
        "    return binary_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JOGXVJrn9RkO"
      },
      "outputs": [],
      "source": [
        "def whiteBlackRatio(img):\n",
        "    h = img.shape[0]\n",
        "    w = img.shape[1]\n",
        "    #initialized at 1 to avoid division by zero\n",
        "    blackCount=1\n",
        "    whiteCount=0\n",
        "    for y in range(0,h):\n",
        "        for x in range (0,w):\n",
        "            if (img[y,x]==0):\n",
        "                blackCount+=1\n",
        "            else:\n",
        "                whiteCount+=1\n",
        "    return whiteCount/blackCount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Q1mjR20t9RkR"
      },
      "outputs": [],
      "source": [
        "def blackPixelsCount(img):\n",
        "    blackCount=1 #initialized at 1 to avoid division by zero when we calculate the ratios\n",
        "    h = img.shape[0]\n",
        "    w = img.shape[1]\n",
        "    for y in range(0,h):\n",
        "        for x in range (0,w):\n",
        "            if (img[y,x]==0):\n",
        "                blackCount+=1\n",
        "\n",
        "    return blackCount"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "plO6A8TN9RkS"
      },
      "outputs": [],
      "source": [
        "def horizontalTransitions(img):\n",
        "    h = img.shape[0]\n",
        "    w = img.shape[1]\n",
        "    maximum=0\n",
        "    for y in range(0,h):\n",
        "        prev=img[y,0]\n",
        "        transitions=0\n",
        "        for x in range (1,w):\n",
        "            if (img[y,x]!=prev):\n",
        "                transitions+=1\n",
        "                prev= img[y,x]\n",
        "        maximum= max(maximum,transitions)\n",
        "\n",
        "    return maximum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BC7-ElnB9RkT"
      },
      "outputs": [],
      "source": [
        "def verticalTransitions(img):\n",
        "    h = img.shape[0]\n",
        "    w = img.shape[1]\n",
        "    maximum=0\n",
        "    for x in range(0,w):\n",
        "        prev=img[0,x]\n",
        "        transitions=0\n",
        "        for y in range (1,h):\n",
        "            if (img[y,x]!=prev):\n",
        "                transitions+=1\n",
        "                prev= img[y,x]\n",
        "        maximum= max(maximum,transitions)\n",
        "\n",
        "    return maximum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "0ilVNjal9RkU"
      },
      "outputs": [],
      "source": [
        "def histogramAndCenterOfMass(img):\n",
        "    h = img.shape[0]\n",
        "    w = img.shape[1]\n",
        "    histogram=[]\n",
        "    sumX=0\n",
        "    sumY=0\n",
        "    num=0\n",
        "    for x in range(0,w):\n",
        "        localHist=0\n",
        "        for y in range (0,h):\n",
        "            if(img[y,x]==0):\n",
        "                sumX+=x\n",
        "                sumY+=y\n",
        "                num+=1\n",
        "                localHist+=1\n",
        "        histogram.append(localHist)\n",
        "\n",
        "    return sumX/num , sumY/num, histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Nfow9-ex9RkW"
      },
      "outputs": [],
      "source": [
        "def number_of_endpoints(img):\n",
        "    # Apply morphological operations to find endpoints\n",
        "    kernel = np.array([[1, 1, 1],\n",
        "                       [1, 10, 1],\n",
        "                       [1, 1, 1]], dtype=np.uint8)\n",
        "    dilated_img = cv.dilate(img, kernel)\n",
        "    endpoints_img = dilated_img - img\n",
        "\n",
        "    # Count the number of endpoints\n",
        "    endpoints_count = np.count_nonzero(endpoints_img)\n",
        "\n",
        "    return endpoints_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IFskTIsr9RkY"
      },
      "outputs": [],
      "source": [
        "def number_of_loops(img):\n",
        "    # Apply edge detection to the image\n",
        "    edges = cv.Canny(img, 50, 150)\n",
        "\n",
        "    # Find contours in the edge-detected image\n",
        "    contours, _ = cv.findContours(edges, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    # Count the number of contours with area greater than a threshold (assuming loops will have larger areas)\n",
        "    loop_count = sum(1 for contour in contours if cv.contourArea(contour) > 100)  # Adjust the threshold as needed\n",
        "\n",
        "    return loop_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kvu9a7cM9Rka"
      },
      "outputs": [],
      "source": [
        "def number_of_line_crossings(img):\n",
        "    # Apply Hough Line Transform to detect lines in the image\n",
        "    lines = cv.HoughLines(img, 1, np.pi/180, 100)\n",
        "\n",
        "    # Count the number of detected lines\n",
        "    line_count = len(lines) if lines is not None else 0\n",
        "\n",
        "    return line_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cI-Wl-V69Rkc"
      },
      "outputs": [],
      "source": [
        "def discrete_wavelet_transform(img):\n",
        "    coeffs = pywt.dwt2(img, 'haar')  # 'haar' is the wavelet family, you can choose another one if needed\n",
        "    LL, (LH, HL, HH) = coeffs  # LL: Approximation, LH: Horizontal detail, HL: Vertical detail, HH: Diagonal detail\n",
        "    return LL.flatten(), LH.flatten(), HL.flatten(), HH.flatten()\n",
        "\n",
        "def discrete_cosine_transform(img):\n",
        "    return dct(dct(img.T, norm='ortho').T, norm='ortho').flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wcuz6rKw9Rke"
      },
      "outputs": [],
      "source": [
        "def fourier_features(img):\n",
        "    img_resized = cv.resize(img, (32,32), interpolation=cv.INTER_AREA)\n",
        "    f_transform = np.fft.fft2(img_resized)\n",
        "    f_shift = np.fft.fftshift(f_transform)\n",
        "    # Add a small constant to avoid log(0)\n",
        "    magnitude_spectrum = 20 * np.log(np.abs(f_shift) + 1e-8)  # 1e-8 is a small number to avoid log(0)\n",
        "    return np.ravel(magnitude_spectrum)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5kMoCkv19Rkf"
      },
      "outputs": [],
      "source": [
        "def gradient_orientation_histogram(img):\n",
        "    gx, gy = np.gradient(img.astype(float))\n",
        "    magnitude = np.sqrt(gx**2 + gy**2)\n",
        "    orientation = np.arctan2(gy, gx) * (180 / np.pi) % 180\n",
        "    histogram, _ = np.histogram(orientation, bins=9, range=(0, 180), weights=magnitude)\n",
        "    return histogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "q1ZvFALO9Rkh"
      },
      "outputs": [],
      "source": [
        "def getFeatures(img):\n",
        "    x,y= img.shape\n",
        "    featuresList=[]\n",
        "    # first feature: height/width ratio\n",
        "    featuresList.append(y/x)\n",
        "    #second feature is ratio between black and white count pixels\n",
        "    featuresList.append(whiteBlackRatio(img))\n",
        "    #third and fourth features are the number of vertical and horizontal transitions\n",
        "    featuresList.append(horizontalTransitions(img))\n",
        "    featuresList.append(verticalTransitions(img))\n",
        "\n",
        "    #print (featuresList)\n",
        "    #splitting the image into 4 images\n",
        "    topLeft=img[0:y//2,0:x//2]\n",
        "    topRight=img[0:y//2,x//2:x]\n",
        "    bottomeLeft=img[y//2:y,0:x//2]\n",
        "    bottomRight=img[y//2:y,x//2:x]\n",
        "\n",
        "    #get white to black ratio in each quarter\n",
        "    featuresList.append(whiteBlackRatio(topLeft))\n",
        "    featuresList.append(whiteBlackRatio(topRight))\n",
        "    featuresList.append(whiteBlackRatio(bottomeLeft))\n",
        "    featuresList.append(whiteBlackRatio(bottomRight))\n",
        "\n",
        "    #the next 6 features are:\n",
        "    #• Black Pixels in Region 1/ Black Pixels in Region 2.\n",
        "    #• Black Pixels in Region 3/ Black Pixels in Region 4.\n",
        "    #• Black Pixels in Region 1/ Black Pixels in Region 3.\n",
        "    #• Black Pixels in Region 2/ Black Pixels in Region 4.\n",
        "    #• Black Pixels in Region 1/ Black Pixels in Region 4\n",
        "    #• Black Pixels in Region 2/ Black Pixels in Region 3.\n",
        "    topLeftCount=blackPixelsCount(topLeft)\n",
        "    topRightCount=blackPixelsCount(topRight)\n",
        "    bottomLeftCount=blackPixelsCount(bottomeLeft)\n",
        "    bottomRightCount=blackPixelsCount(bottomRight)\n",
        "\n",
        "    featuresList.append(topLeftCount/topRightCount)\n",
        "    featuresList.append(bottomLeftCount/bottomRightCount)\n",
        "    featuresList.append(topLeftCount/bottomLeftCount)\n",
        "    featuresList.append(topRightCount/bottomRightCount)\n",
        "    featuresList.append(topLeftCount/bottomRightCount)\n",
        "    featuresList.append(topRightCount/bottomLeftCount)\n",
        "    #get center of mass and horizontal histogram\n",
        "    xCenter, yCenter,xHistogram =histogramAndCenterOfMass(img)\n",
        "    featuresList.append(xCenter)\n",
        "    featuresList.append(yCenter)\n",
        "    #featuresList.extend(xHistogram)\n",
        "    #print(len(featuresList))\n",
        "\n",
        "\n",
        "    # Structural features\n",
        "    featuresList.append(number_of_loops(img))\n",
        "    featuresList.append(number_of_line_crossings(img))\n",
        "    featuresList.append(number_of_endpoints(img))\n",
        "\n",
        "    # Transform-based features\n",
        "    #dwt_features = discrete_wavelet_transform(img)\n",
        "    #dct_features = discrete_cosine_transform(img)\n",
        "    #featuresList.append(dwt_features)\n",
        "    #featuresList.append(dct_features)\n",
        "\n",
        "    featuresList.extend(gradient_orientation_histogram(img))\n",
        "    featuresList.extend(fourier_features(img))\n",
        "\n",
        "    return featuresList"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "2hPnNTAS9RlF"
      },
      "outputs": [],
      "source": [
        "def get_immediate_subdirectories(a_dir):\n",
        "    return [name for name in os.listdir(a_dir)\n",
        "        if os.path.isdir(os.path.join(a_dir, name))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LYjsZowa9RlG"
      },
      "outputs": [],
      "source": [
        "def getListOfFiles(dirName):\n",
        "    # create a list of file and sub directories\n",
        "    # names in the given directory\n",
        "    listOfFile = os.listdir(dirName)\n",
        "    allFiles = list()\n",
        "    # Iterate over all the entries\n",
        "    for entry in listOfFile:\n",
        "        # Create full path\n",
        "        fullPath = os.path.join(dirName, entry)\n",
        "        # If entry is a directory then get the list of files in this directory\n",
        "        if os.path.isdir(fullPath):\n",
        "            allFiles = allFiles + getListOfFiles(fullPath)\n",
        "        else:\n",
        "            allFiles.append(fullPath)\n",
        "\n",
        "    return allFiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5wph97rU9RlH"
      },
      "outputs": [],
      "source": [
        "def write_to_csv(y_test, y_pred, filename):\n",
        "    with open(filename, 'w', newline='') as csvfile:\n",
        "        writer = csv.writer(csvfile)\n",
        "        writer.writerow(['y_test', 'y_pred'])\n",
        "        for test, pred in zip(y_test, y_pred):\n",
        "            writer.writerow([test, pred])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BQTIkWGa9RlH"
      },
      "outputs": [],
      "source": [
        "def accuracyy(y_test,y_pred):\n",
        "    y_test_char=[]\n",
        "    y_pred_char=[]\n",
        "    charLabels = ['alif', 'ba', 'ta', 'tha', 'gim', 'ha', 'kha', 'dal' ,'thal', 'ra', 'zay', 'sin', 'shin', 'sad', 'dad', 'tah', 'za', 'ayn', 'gayn', 'fa', 'qaf', 'kaf', 'lam', 'mim', 'non', 'haa', 'waw', 'ya', 'hamza']\n",
        "    charPositions = ['Beginning', 'Middle', 'End', 'Isolated']\n",
        "    for test in y_test:\n",
        "        for i in range(len(charLabels)):\n",
        "            for j in range(len(charPositions)):\n",
        "                if test==charLabels[i]+charPositions[j]:\n",
        "                    y_test_char.append(charLabels[i])\n",
        "    for test in y_pred:\n",
        "        for i in range(len(charLabels)):\n",
        "            for j in range(len(charPositions)):\n",
        "                if test==charLabels[i]+charPositions[j]:\n",
        "                    y_pred_char.append(charLabels[i])\n",
        "    return accuracy_score(y_test_char, y_pred_char)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "wqQdLzXS9RlI"
      },
      "outputs": [],
      "source": [
        "def trainAndClassify(data, classes, classifiers):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(data, classes, test_size=0.30)\n",
        "    accuracies = []\n",
        "\n",
        "    for idx, clf in enumerate(classifiers):\n",
        "        clf.fit(X_train, y_train)\n",
        "        y_pred = clf.predict(X_test)\n",
        "        accuracy = accuracyy(y_test, y_pred)\n",
        "        accuracies.append(accuracy)\n",
        "        write_to_csv(y_test, y_pred, 'predictions'+str(idx)+'.csv')\n",
        "        joblib.dump(clf, f'classifier{idx}.pkl')\n",
        "    return accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Vy_UsA4h9RlJ"
      },
      "outputs": [],
      "source": [
        "def removeMargins(img):\n",
        "    th, threshed = cv.threshold(img, 245, 255, cv.THRESH_BINARY_INV)\n",
        "    ## (2) Morph-op to remove noise\n",
        "    kernel = cv.getStructuringElement(cv.MORPH_ELLIPSE, (11,11))\n",
        "    morphed = cv.morphologyEx(threshed, cv.MORPH_CLOSE, kernel)\n",
        "    ## (3) Find the max-area contour\n",
        "    cnts = cv.findContours(morphed, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)[-2]\n",
        "    cnt = sorted(cnts, key=cv.contourArea)[-1]\n",
        "    ## (4) Crop and save it\n",
        "    x,y,w,h = cv.boundingRect(cnt)\n",
        "    dst = img[y:y+h, x:x+w]\n",
        "    return dst"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "\n",
        "def analyze_image_sizes(directory):\n",
        "    dimensions = []\n",
        "    for root, dirs, files in os.walk(directory):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('png', 'jpg', 'jpeg')):\n",
        "                path = os.path.join(root, file)\n",
        "                img = cv2.imread(path)\n",
        "                if img is not None:\n",
        "                    dimensions.append(img.shape[:2])  # (height, width)\n",
        "                else:\n",
        "                    print(f\"Warning: Failed to load image {path}\")\n",
        "    return dimensions\n",
        "directory = './drive/MyDrive/LettersDataset'\n",
        "image_sizes = analyze_image_sizes(directory)\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "size_counts = Counter(image_sizes)\n",
        "most_common_size = size_counts.most_common(1)[0][0]\n",
        "print(f\"The most common image size is: {most_common_size}\")\n",
        "\n",
        "average_height = int(sum(dim[0] for dim in image_sizes) / len(image_sizes))\n",
        "average_width = int(sum(dim[1] for dim in image_sizes) / len(image_sizes))\n",
        "print(f\"The average image size is: {average_height} x {average_width}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8D93No5UbyF",
        "outputId": "bf91c168-c737-4445-d27b-af9f0918256c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The most common image size is: (32, 32)\n",
            "The average image size is: 32 x 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "IL7GvBG89RlK"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "classes = []\n",
        "directory = './drive/MyDrive/LettersDataset'\n",
        "chars = get_immediate_subdirectories(directory)\n",
        "numOfFeatures = 1052\n",
        "count=0\n",
        "charPositions=['Beginning','End','Isolated','Middle']\n",
        "for char in chars:\n",
        "    for position in charPositions:\n",
        "        path = os.path.join(directory, char, position)\n",
        "        if os.path.isdir(path):\n",
        "            listOfFiles = getListOfFiles(path)\n",
        "            for filename in listOfFiles:\n",
        "                img = cv.imread(filename)\n",
        "                if img is None:\n",
        "                    continue\n",
        "                gray_img = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
        "                cropped = removeMargins(gray_img)\n",
        "                binary_img = binary_otsus(gray_img, 0)\n",
        "                features = getFeatures(binary_img)\n",
        "                if len(features) == numOfFeatures:\n",
        "                    data.append(features)\n",
        "                    classes.append(char + position)\n",
        "                else:\n",
        "                    print(f\"Feature length mismatch in file {filename}, got {len(features)} features\")\n",
        "\n",
        "# Zip the data and classes together\n",
        "zipped = list(zip(data, classes))\n",
        "#random.seed(42)  # Use any number to seed the generator\n",
        "\n",
        "# Shuffle the combined list\n",
        "random.shuffle(zipped)\n",
        "\n",
        "# Unzip back into data and classes\n",
        "data, classes = zip(*zipped)\n",
        "\n",
        "# Now convert lists to numpy arrays and reshape as necessary\n",
        "data = np.array(data).reshape(-1, numOfFeatures)\n",
        "classes = np.array(classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "KLl3P2DE9RlL",
        "outputId": "0d448442-b3b6-462a-a615-4d12a77dc5e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:693: UserWarning: Training interrupted by user.\n",
            "  warnings.warn(\"Training interrupted by user.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Classifier 1: 0.5674418604651162\n",
            "Accuracy of Classifier 2: 0.1648578811369509\n",
            "Accuracy of Classifier 3: 0.6211886304909561\n",
            "Accuracy of Classifier 4: 0.7607235142118863\n",
            "Accuracy of Classifier 5: 0.5989664082687338\n",
            "Accuracy of Classifier 6: 0.16434108527131783\n",
            "Accuracy of Classifier 7: 0.5369509043927648\n",
            "Accuracy of Classifier 8: 0.6470284237726098\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Accuracy of Classifier 1: 0.033926155035762615\\nAccuracy of Classifier 2: 0.2656098975449449\\nAccuracy of Classifier 3: 0.2718925188478639\\nAccuracy of Classifier 4: 0.20722984728397448\\nAccuracy of Classifier 5: 0.09133964817320704\\nAccuracy of Classifier 6: 0.11946646046781365\\nAccuracy of Classifier 7: 0.2678329789290547\\nAccuracy of Classifier 8: 0.14305045428184807'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "#calssifiers test\n",
        "classifiers = [ svm.LinearSVC(),\n",
        "                MLPClassifier(alpha=1e-4, hidden_layer_sizes=(100,), max_iter=1000),\n",
        "                MLPClassifier(alpha=1e-4, hidden_layer_sizes=(500,200), max_iter=2000),\n",
        "                MLPClassifier(alpha=5, hidden_layer_sizes=(500, 200,), max_iter=2000),\n",
        "                MLPClassifier(alpha=50, hidden_layer_sizes=(500, 200,), max_iter=2000),\n",
        "                MLPClassifier(alpha=20, hidden_layer_sizes=(100,), max_iter=1000),\n",
        "                MLPClassifier(alpha=0, hidden_layer_sizes=(1000,500), max_iter=1000),\n",
        "                GaussianNB()]\n",
        "\n",
        "accuracies = trainAndClassify(data, classes, classifiers)\n",
        "for idx, accuracy in enumerate(accuracies):\n",
        "    print(f\"Accuracy of Classifier {idx + 1}: {accuracy}\")\n",
        "\n",
        "'''Accuracy of Classifier 1: 0.033926155035762615\n",
        "Accuracy of Classifier 2: 0.2656098975449449\n",
        "Accuracy of Classifier 3: 0.2718925188478639\n",
        "Accuracy of Classifier 4: 0.20722984728397448\n",
        "Accuracy of Classifier 5: 0.09133964817320704\n",
        "Accuracy of Classifier 6: 0.11946646046781365\n",
        "Accuracy of Classifier 7: 0.2678329789290547\n",
        "Accuracy of Classifier 8: 0.14305045428184807'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMlEXhne9RlM",
        "outputId": "fc98ebbd-9e0f-4779-ff76-14652f476187"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Classifier 1: 0.17622739018087855\n",
            "Accuracy of Classifier 2: 0.765891472868217\n"
          ]
        }
      ],
      "source": [
        "#calssifiers test\n",
        "classifiers = [\n",
        "    MLPClassifier(alpha=1e-3, hidden_layer_sizes=(50,), max_iter=1000),\n",
        "    MLPClassifier(alpha=1e-6, hidden_layer_sizes=(300, 200,), max_iter=1000),\n",
        "]\n",
        "\n",
        "accuracies = trainAndClassify(data, classes, classifiers)\n",
        "for idx, accuracy in enumerate(accuracies):\n",
        "    print(f\"Accuracy of Classifier {idx + 1}: {accuracy}\")\n",
        "    '''\n",
        "Accuracy of Classifier 1: 0.34805722018171276\n",
        "Accuracy of Classifier 2: 0.33220568335588635'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "F-4-Vb0y9RlM",
        "outputId": "de1c3eb2-def9-476a-d462-8a7b2046dc7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Classifier 1: 0.39749575551782684\n",
            "Accuracy of Classifier 2: 0.34465195246179964\n",
            "Accuracy of Classifier 3: 0.5785229202037352\n",
            "Accuracy of Classifier 4: 0.698641765704584\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Accuracy of Classifier 1: 0.4087870105062082\\nAccuracy of Classifier 2: 0.3390639923591213\\nAccuracy of Classifier 3: 0.6160458452722063\\nAccuracy of Classifier 4: 0.7163323782234957\\nAccuracy of Classifier 1: 0.18325923062052968'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "#classifiers test\n",
        "classifiers = [\n",
        "    SVC(),  # Support Vector Classifier\n",
        "    KNeighborsClassifier(),  # k-Nearest Neighbors\n",
        "    DecisionTreeClassifier(),  # Decision Tree\n",
        "    RandomForestClassifier(),  # Random Forest\n",
        "]\n",
        "accuracies = trainAndClassify(data, classes,classifiers)\n",
        "for idx, accuracy in enumerate(accuracies):\n",
        "    print(f\"Accuracy of Classifier {idx + 1}: {accuracy}\")\n",
        "'''Accuracy of Classifier 1: 0.4087870105062082\n",
        "Accuracy of Classifier 2: 0.3390639923591213\n",
        "Accuracy of Classifier 3: 0.6160458452722063\n",
        "Accuracy of Classifier 4: 0.7163323782234957\n",
        "Accuracy of Classifier 1: 0.18325923062052968'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCX3QgF29RlN",
        "outputId": "0fe372ed-492a-4e37-b10c-ebe301375561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Classifier 1: 0.6865449915110357\n"
          ]
        }
      ],
      "source": [
        "classifiers = [\n",
        "    RandomForestClassifier(),\n",
        "]\n",
        "\n",
        "accuracies = trainAndClassify(data, classes,classifiers)\n",
        "for idx, accuracy in enumerate(accuracies):\n",
        "    print(f\"Accuracy of Classifier {idx + 1}: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4obqz1_29RlN",
        "outputId": "efc5a279-d478-49d4-9175-218cdf664b69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction for letter 1: sinIsolated\n",
            "Prediction for letter 2: yaIsolated\n",
            "Prediction for letter 3: taIsolated\n",
            "Prediction for letter 4: thaIsolated\n",
            "Prediction for letter 5: taIsolated\n",
            "سيتثت\n"
          ]
        }
      ],
      "source": [
        "#directory of hamza and alif can edit yaa too\n",
        "#handle spaces bet words\n",
        "chars = ['ب', 'ت', 'ث', 'ج', 'ح', 'خ', 'د', 'ذ', 'ر', 'ز', 'س', 'ش', 'ص', 'ض', 'ط', 'ظ', 'ع', 'غ', 'ف', 'ق','ك', 'ل', 'م', 'ن', 'ه', 'و','ي']\n",
        "charLabels =['ba', 'ta', 'tha', 'gim', 'ha', 'kha', 'dal' ,'thal', 'ra', 'zay', 'sin', 'shin', 'sad', 'dad', 'tah', 'za', 'ayn', 'gayn', 'fa', 'qaf', 'kaf', 'lam', 'mim', 'non', 'haa', 'waw', 'ya']\n",
        "positionsLabels=['Beginning','End','Isolated','Middle']\n",
        "word = ''\n",
        "def labeltochar(label):\n",
        "    if label=='alifMiddle'or label=='alifEnd':\n",
        "        return 'ا'\n",
        "    if label=='alifBeginning'or label=='alifIsolated':\n",
        "        return 'أ'\n",
        "    if label=='hamzaEnd':\n",
        "        return 'ئ'\n",
        "    if label=='hamzaBeginning'or label=='hamzaIsolated':\n",
        "        return 'ء'\n",
        "    if label=='hamzaMiddle':\n",
        "        return 'ؤ'\n",
        "\n",
        "    for i in range(len(charLabels)):\n",
        "        for j in range(len(positionsLabels)):\n",
        "            if label==charLabels[i]+positionsLabels[j]:\n",
        "                return chars[i]\n",
        "\n",
        "classifier = joblib.load('classifier0.pkl')\n",
        "folder= getListOfFiles('Test')\n",
        "count=0\n",
        "data1 = []\n",
        "for file in folder:\n",
        "    img = cv.imread(file)\n",
        "    img_resized = cv.resize(img, (32,32), interpolation=cv.INTER_AREA)\n",
        "    gray_img = cv.cvtColor(img_resized, cv.COLOR_BGR2GRAY)\n",
        "    cropped = removeMargins(gray_img)\n",
        "    binary_img = binary_otsus(gray_img, 0)\n",
        "    features = getFeatures(binary_img)\n",
        "    data1.append(features)\n",
        "    data2 = np.array(data1).reshape(-1, numOfFeatures)\n",
        "    prediction = classifier.predict(data2)\n",
        "    count=count+1\n",
        "    print(f\"Prediction for letter {count}: {prediction[0]}\")\n",
        "    #cv.imshow('image',binary_img)\n",
        "    char=labeltochar(prediction[0])\n",
        "    word=word+char\n",
        "    data1 = []\n",
        "\n",
        "print(word)\n",
        "    #increase dataset\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}